name: Deploy EKS Cluster and Argo CD

on:
  push:
    branches:
      - main
  workflow_dispatch:

permissions:
  id-token: write # Required for OIDC
  contents: read

env:
  AWS_REGION: ${{ vars.AWS_REGION || 'us-east-1' }}
  CLUSTER_NAME: ${{ vars.CLUSTER_NAME || 'logicall-ai-cluster' }}
  AWS_ROLE_ARN: ${{ vars.AWS_ROLE_ARN }}

jobs:
  deploy:
    runs-on: ubuntu-latest
    environment: AWS
    if: github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.AWS_ROLE_ARN }}
          role-session-name: GitHubActions-Deploy
          aws-region: ${{ env.AWS_REGION }}
          
      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0

      - name: Terraform Init - EKS
        working-directory: infra/eks
        run: terraform init

      - name: Import existing resources (if any)
        working-directory: infra/eks
        continue-on-error: true
        run: |
          # Import CloudWatch log group
          terraform import aws_cloudwatch_log_group.cluster /aws/eks/${{ env.CLUSTER_NAME }}/cluster || echo "Log group already in state"
          
          # Import IAM roles
          terraform import aws_iam_role.cluster ${{ env.CLUSTER_NAME }}-cluster-role || echo "Cluster role already in state"
          terraform import aws_iam_role.node_group ${{ env.CLUSTER_NAME }}-node-group-role || echo "Node group role already in state"
          
          # Import EKS cluster
          terraform import aws_eks_cluster.main ${{ env.CLUSTER_NAME }} || echo "Cluster already in state"
          
          # Import VPC resources if cluster exists
          if aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }} --query 'cluster.status' --output text 2>/dev/null | grep -q "ACTIVE"; then
            echo "Cluster exists, importing VPC resources..."
            VPC_ID=$(aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }} --query 'cluster.resourcesVpcConfig.vpcId' --output text)
            
            # Import VPC
            terraform import 'module.vpc.aws_vpc.this[0]' $VPC_ID || echo "VPC already in state"
            
            # Import subnets (matching by CIDR and AZ)
            for SUBNET_ID in $(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" --region ${{ env.AWS_REGION }} --query 'Subnets[*].SubnetId' --output text); do
              SUBNET_INFO=$(aws ec2 describe-subnets --subnet-ids $SUBNET_ID --region ${{ env.AWS_REGION }} --query 'Subnets[0].[AvailabilityZone,CidrBlock,Tags[?Key==`Name`].Value|[0]]' --output text)
              AZ=$(echo $SUBNET_INFO | cut -f1)
              CIDR=$(echo $SUBNET_INFO | cut -f2)
              NAME=$(echo $SUBNET_INFO | cut -f3)
              
              if [[ "$NAME" == *"public-us-east-1a"* ]] || [[ "$CIDR" == "10.0.1.0/24" ]]; then
                terraform import 'module.vpc.aws_subnet.public[0]' $SUBNET_ID || echo "Public subnet 0 already in state"
              elif [[ "$NAME" == *"public-us-east-1b"* ]] || [[ "$CIDR" == "10.0.2.0/24" ]]; then
                terraform import 'module.vpc.aws_subnet.public[1]' $SUBNET_ID || echo "Public subnet 1 already in state"
              elif [[ "$NAME" == *"private-us-east-1a"* ]] || [[ "$CIDR" == "10.0.10.0/24" ]]; then
                terraform import 'module.vpc.aws_subnet.private[0]' $SUBNET_ID || echo "Private subnet 0 already in state"
              elif [[ "$NAME" == *"private-us-east-1b"* ]] || [[ "$CIDR" == "10.0.20.0/24" ]]; then
                terraform import 'module.vpc.aws_subnet.private[1]' $SUBNET_ID || echo "Private subnet 1 already in state"
              fi
            done
            
            # Import Internet Gateway
            IGW_ID=$(aws ec2 describe-internet-gateways --filters "Name=attachment.vpc-id,Values=$VPC_ID" --region ${{ env.AWS_REGION }} --query 'InternetGateways[0].InternetGatewayId' --output text)
            if [ "$IGW_ID" != "None" ] && [ -n "$IGW_ID" ]; then
              terraform import 'module.vpc.aws_internet_gateway.this[0]' $IGW_ID || echo "IGW already in state"
            fi
            
            # Import NAT Gateway and EIP
            NAT_INFO=$(aws ec2 describe-nat-gateways --filter "Name=vpc-id,Values=$VPC_ID" --region ${{ env.AWS_REGION }} --query 'NatGateways[0].[NatGatewayId,NatGatewayAddresses[0].AllocationId]' --output text 2>/dev/null || echo "None None")
            if [ "$NAT_INFO" != "None None" ] && [ -n "$NAT_INFO" ]; then
              NAT_ID=$(echo $NAT_INFO | cut -f1)
              EIP_ALLOC=$(echo $NAT_INFO | cut -f2)
              if [ "$EIP_ALLOC" != "None" ] && [ -n "$EIP_ALLOC" ]; then
                terraform import 'module.vpc.aws_eip.nat[0]' $EIP_ALLOC || echo "EIP already in state"
              fi
              terraform import 'module.vpc.aws_nat_gateway.this[0]' $NAT_ID || echo "NAT Gateway already in state"
            fi
          fi
          
          # Refresh state
          terraform refresh || true

      - name: Check Terraform plan for destructive changes
        id: terraform-plan
        working-directory: infra/eks
        continue-on-error: true
        run: |
          PLAN_OUTPUT=$(terraform plan -no-color 2>&1 || true)
          echo "$PLAN_OUTPUT" > plan_output.txt
          
          # Check if plan wants to destroy the cluster
          if echo "$PLAN_OUTPUT" | grep -q "aws_eks_cluster.main.*will be destroyed"; then
            echo "destructive=true" >> $GITHUB_OUTPUT
            echo "⚠️ WARNING: Terraform wants to destroy the cluster!"
            echo "This usually means VPC resources weren't imported."
            echo "Aborting to prevent cluster destruction."
            exit 1
          else
            echo "destructive=false" >> $GITHUB_OUTPUT
            echo "Plan looks safe, no cluster destruction detected"
          fi

      - name: Terraform Apply - EKS
        working-directory: infra/eks
        if: steps.terraform-plan.outputs.destructive != 'true'
        timeout-minutes: 45
        run: terraform apply -auto-approve

      - name: Configure kubectl
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}
          kubectl cluster-info

      - name: Setup Helm
        uses: azure/setup-helm@v3
        with:
          version: '3.13.0'

      - name: Install AWS Load Balancer Controller
        run: |
          # Note: IAM role for AWS Load Balancer Controller should be created via Terraform
          # This step installs the controller using Helm
          helm repo add eks https://aws.github.io/eks-charts
          helm repo update
          
          # Create service account (IAM role will be attached via Terraform)
          kubectl create serviceaccount aws-load-balancer-controller -n kube-system --dry-run=client -o yaml | kubectl apply -f -
          
          helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
            -n kube-system \
            --set clusterName=${{ env.CLUSTER_NAME }} \
            --set serviceAccount.create=false \
            --set serviceAccount.name=aws-load-balancer-controller \
            --set region=${{ env.AWS_REGION }} \
            --wait

      - name: Install External Secrets Operator
        run: |
          helm repo add external-secrets https://charts.external-secrets.io
          helm repo update
          
          helm upgrade --install external-secrets external-secrets/external-secrets \
            -n external-secrets \
            --create-namespace \
            --set installCRDs=true \
            --wait

      - name: Install Argo CD
        run: |
          kubectl create namespace argocd --dry-run=client -o yaml | kubectl apply -f -
          kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
          
          echo "Waiting for Argo CD to be ready..."
          kubectl wait --for=condition=available deployment/argocd-server -n argocd --timeout=300s
          
          echo "Exposing Argo CD UI via LoadBalancer..."
          kubectl patch svc argocd-server -n argocd -p '{"spec": {"type": "LoadBalancer"}}'

      - name: Apply Argo CD root application
        run: |
          echo "Applying root-app Argo CD application..."
          kubectl apply -f gitops/argocd/root-app.yaml
          
          echo "Waiting for root-app to sync..."
          # Give Argo CD a moment to pick up the new file
          sleep 20
          kubectl wait --for=condition=healthy application/root-app -n argocd --timeout=300s || echo "Wait timed out, but proceeding to status check..."

      - name: Final Status and Credentials
        run: |
          echo "--- Deployment Summary ---"
          echo "Argo CD URL:"
          kubectl get svc argocd-server -n argocd -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'
          echo ""
          echo "Argo CD Admin Password:"
          kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d
          echo ""
          echo "Nginx App URL:"
          kubectl get svc nginx-hello -n apps -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' || echo "Pending..."