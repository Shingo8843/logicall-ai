name: Deploy EKS Cluster and Argo CD

on:
  push:
    branches:
      - main
  workflow_dispatch:
    inputs:
      destroy_first:
        description: 'Destroy existing resources before creating new ones'
        required: false
        default: 'false'
        type: boolean

permissions:
  id-token: write # Required for OIDC
  contents: read

env:
  AWS_REGION: ${{ vars.AWS_REGION || 'us-east-1' }}
  CLUSTER_NAME: ${{ vars.CLUSTER_NAME || 'logicall-ai-cluster' }}
  AWS_ROLE_ARN: ${{ vars.AWS_ROLE_ARN }}

jobs:
  deploy:
    runs-on: ubuntu-latest
    environment: AWS
    if: github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.AWS_ROLE_ARN }}
          role-session-name: GitHubActions-Deploy
          aws-region: ${{ env.AWS_REGION }}
          
      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0

      - name: Terraform Init - EKS
        working-directory: infra/eks
        run: |
          terraform init
          # If migrating from local to S3 backend, migrate state
          terraform init -migrate-state -input=false || echo "Backend already configured or no migration needed"
          
      - name: Clean up stale state (if cluster doesn't exist)
        working-directory: infra/eks
        continue-on-error: true
        run: |
          echo "=== Checking for stale state ==="
          CLUSTER_EXISTS=$(aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }} --query 'cluster.status' --output text 2>&1 || echo "NOT_FOUND")
          
          if [ "$CLUSTER_EXISTS" != "ACTIVE" ]; then
            echo "Cluster does not exist. Cleaning up stale Kubernetes resources from state..."
            # Remove Kubernetes resources from state if they exist
            terraform state rm kubernetes_config_map_v1_data.aws_auth 2>/dev/null || echo "kubernetes_config_map_v1_data.aws_auth not in state"
            terraform state rm data.kubernetes_config_map_v1.aws_auth 2>/dev/null || echo "data.kubernetes_config_map_v1.aws_auth not in state"
            terraform state rm data.aws_eks_cluster_auth.main 2>/dev/null || echo "data.aws_eks_cluster_auth.main not in state"
            echo "State cleanup completed"
          else
            echo "Cluster exists, skipping state cleanup"
          fi

      - name: Destroy existing resources (if requested)
        if: github.event.inputs.destroy_first == 'true'
        working-directory: infra/eks
        continue-on-error: true
        run: |
          echo "=== Destroying existing resources ==="
          CLUSTER_EXISTS=$(aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }} --query 'cluster.status' --output text 2>/dev/null || echo "NOT_FOUND")
          if [ "$CLUSTER_EXISTS" = "ACTIVE" ]; then
            echo "Cluster exists. Destroying all resources..."
            terraform destroy -auto-approve -lock=false || echo "Destroy completed or failed"
            echo "Waiting 30 seconds for resources to be fully deleted..."
            sleep 30
          else
            echo "No existing cluster found, nothing to destroy"
          fi

      - name: Import existing resources (if any)
        working-directory: infra/eks
        continue-on-error: true
        run: |
          echo "=== Importing existing resources ==="
          
          # Check if cluster exists - import it FIRST to satisfy provider dependencies
          CLUSTER_STATUS=$(aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }} --query 'cluster.status' --output text 2>/dev/null || echo "NOT_FOUND")
          
          if [ "$CLUSTER_STATUS" = "ACTIVE" ]; then
            echo "Cluster is ACTIVE - importing cluster FIRST (required for Kubernetes provider)..."
            
            # Import cluster first - this allows Kubernetes provider to initialize properly
            if terraform import -lock=false aws_eks_cluster.main "${{ env.CLUSTER_NAME }}" 2>&1; then
              echo "✓ EKS cluster imported successfully"
            elif terraform state show aws_eks_cluster.main >/dev/null 2>&1; then
              echo "✓ EKS cluster already in state"
            else
              echo "⚠ Cluster import failed, but continuing with other resources..."
            fi
          fi
          
          # Function to import and verify
          import_and_verify() {
            local resource=$1
            local import_id=$2
            local resource_name=$3
            
            echo "Importing $resource_name..."
            IMPORT_OUTPUT=$(terraform import -lock=false $resource $import_id 2>&1)
            IMPORT_EXIT=$?
            
            if [ $IMPORT_EXIT -eq 0 ]; then
              echo "✓ $resource_name imported successfully"
              return 0
            else
              # Check if already in state
              if terraform state show $resource >/dev/null 2>&1; then
                echo "✓ $resource_name already in state"
                return 0
              else
                echo "✗ $resource_name import failed:"
                echo "$IMPORT_OUTPUT" | head -5
                return 1
              fi
            fi
          }
          
          # Import CloudWatch log group
          import_and_verify aws_cloudwatch_log_group.cluster "/aws/eks/${{ env.CLUSTER_NAME }}/cluster" "Log group" || true
          
          # Import IAM roles
          import_and_verify aws_iam_role.cluster "${{ env.CLUSTER_NAME }}-cluster-role" "Cluster IAM role" || true
          import_and_verify aws_iam_role.node_group "${{ env.CLUSTER_NAME }}-node-group-role" "Node group IAM role" || true
          
          # Continue with VPC imports if cluster exists
          if [ "$CLUSTER_STATUS" = "ACTIVE" ]; then
            
            # Get VPC ID from cluster
            VPC_ID=$(aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }} --query 'cluster.resourcesVpcConfig.vpcId' --output text)
            echo "VPC ID: $VPC_ID"
            
            # Import VPC
            import_and_verify 'module.vpc.aws_vpc.this[0]' "$VPC_ID" "VPC" || true
            
            # Import subnets by CIDR block
            echo "Importing subnets..."
            for SUBNET_ID in $(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" --region ${{ env.AWS_REGION }} --query 'Subnets[*].SubnetId' --output text); do
              CIDR=$(aws ec2 describe-subnets --subnet-ids $SUBNET_ID --region ${{ env.AWS_REGION }} --query 'Subnets[0].CidrBlock' --output text)
              case "$CIDR" in
                "10.0.1.0/24")
                  import_and_verify 'module.vpc.aws_subnet.public[0]' "$SUBNET_ID" "Public subnet 0" || true
                  ;;
                "10.0.2.0/24")
                  import_and_verify 'module.vpc.aws_subnet.public[1]' "$SUBNET_ID" "Public subnet 1" || true
                  ;;
                "10.0.10.0/24")
                  import_and_verify 'module.vpc.aws_subnet.private[0]' "$SUBNET_ID" "Private subnet 0" || true
                  ;;
                "10.0.20.0/24")
                  import_and_verify 'module.vpc.aws_subnet.private[1]' "$SUBNET_ID" "Private subnet 1" || true
                  ;;
              esac
            done
            
            # Import Internet Gateway
            IGW_ID=$(aws ec2 describe-internet-gateways --filters "Name=attachment.vpc-id,Values=$VPC_ID" --region ${{ env.AWS_REGION }} --query 'InternetGateways[0].InternetGatewayId' --output text 2>/dev/null)
            if [ "$IGW_ID" != "None" ] && [ -n "$IGW_ID" ]; then
              import_and_verify 'module.vpc.aws_internet_gateway.this[0]' "$IGW_ID" "Internet Gateway" || true
            fi
            
            # Import NAT Gateway and EIP
            NAT_GW=$(aws ec2 describe-nat-gateways --filter "Name=vpc-id,Values=$VPC_ID" --region ${{ env.AWS_REGION }} --query 'NatGateways[0]' --output json 2>/dev/null)
            if [ "$NAT_GW" != "null" ] && [ -n "$NAT_GW" ]; then
              NAT_ID=$(echo $NAT_GW | jq -r '.NatGatewayId' 2>/dev/null || echo "")
              EIP_ALLOC=$(echo $NAT_GW | jq -r '.NatGatewayAddresses[0].AllocationId' 2>/dev/null || echo "")
              if [ -n "$EIP_ALLOC" ] && [ "$EIP_ALLOC" != "null" ]; then
                import_and_verify 'module.vpc.aws_eip.nat[0]' "$EIP_ALLOC" "EIP" || true
              fi
              if [ -n "$NAT_ID" ] && [ "$NAT_ID" != "null" ]; then
                import_and_verify 'module.vpc.aws_nat_gateway.this[0]' "$NAT_ID" "NAT Gateway" || true
              fi
            fi
          else
            echo "Cluster not found or not ACTIVE (status: $CLUSTER_STATUS), skipping cluster and VPC imports"
          fi
          
          echo ""
          echo "=== Final state verification ==="
          terraform state list | grep -E "(aws_cloudwatch_log_group|aws_iam_role|aws_eks_cluster|module.vpc)" || echo "No matching resources found in state"

      - name: Verify imports and check Terraform plan
        id: terraform-plan
        working-directory: infra/eks
        continue-on-error: true
        run: |
          echo "=== Final state verification ==="
          echo "Checking if critical resources are in state..."
          
          # Check each resource individually
          if terraform state show aws_cloudwatch_log_group.cluster >/dev/null 2>&1; then
            echo "✓ Log group is in state"
          else
            echo "✗ Log group NOT in state - will try to create (may fail)"
          fi
          
          if terraform state show aws_iam_role.cluster >/dev/null 2>&1; then
            echo "✓ Cluster IAM role is in state"
          else
            echo "✗ Cluster IAM role NOT in state - will try to create (may fail)"
          fi
          
          if terraform state show aws_iam_role.node_group >/dev/null 2>&1; then
            echo "✓ Node group IAM role is in state"
          else
            echo "✗ Node group IAM role NOT in state - will try to create (may fail)"
          fi
          
          if terraform state show aws_eks_cluster.main >/dev/null 2>&1; then
            echo "✓ EKS cluster is in state"
          else
            echo "✗ EKS cluster NOT in state"
          fi
          
          if terraform state show 'module.vpc.aws_vpc.this[0]' >/dev/null 2>&1; then
            echo "✓ VPC is in state"
          else
            echo "✗ VPC NOT in state - will try to create (may hit VPC limit)"
          fi
          
          echo ""
          echo "Running terraform plan..."
          PLAN_OUTPUT=$(terraform plan -no-color 2>&1 || true)
          echo "$PLAN_OUTPUT"
          
          # Check if plan wants to destroy the cluster
          if echo "$PLAN_OUTPUT" | grep -q "aws_eks_cluster.main.*will be destroyed"; then
            echo "destructive=true" >> $GITHUB_OUTPUT
            echo "⚠️ ERROR: Terraform wants to destroy the cluster!"
            echo "Aborting to prevent cluster destruction."
            exit 1
          fi
          
          echo "destructive=false" >> $GITHUB_OUTPUT
          echo "Plan check completed"

      - name: Terraform Apply - EKS
        working-directory: infra/eks
        if: steps.terraform-plan.outputs.destructive != 'true'
        timeout-minutes: 45
        run: terraform apply -auto-approve

      - name: Configure kubectl
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}
          kubectl cluster-info

      - name: Setup Helm
        uses: azure/setup-helm@v3
        with:
          version: '3.13.0'

      - name: Install AWS Load Balancer Controller
        run: |
          # Note: IAM role for AWS Load Balancer Controller should be created via Terraform
          # This step installs the controller using Helm
          helm repo add eks https://aws.github.io/eks-charts
          helm repo update
          
          # Create service account (IAM role will be attached via Terraform)
          kubectl create serviceaccount aws-load-balancer-controller -n kube-system --dry-run=client -o yaml | kubectl apply -f -
          
          helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
            -n kube-system \
            --set clusterName=${{ env.CLUSTER_NAME }} \
            --set serviceAccount.create=false \
            --set serviceAccount.name=aws-load-balancer-controller \
            --set region=${{ env.AWS_REGION }} \
            --wait

      - name: Install External Secrets Operator
        run: |
          helm repo add external-secrets https://charts.external-secrets.io
          helm repo update
          
          helm upgrade --install external-secrets external-secrets/external-secrets \
            -n external-secrets \
            --create-namespace \
            --set installCRDs=true \
            --wait

      - name: Install Argo CD
        run: |
          kubectl create namespace argocd --dry-run=client -o yaml | kubectl apply -f -
          kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
          
          echo "Waiting for Argo CD to be ready..."
          kubectl wait --for=condition=available deployment/argocd-server -n argocd --timeout=300s
          
          echo "Exposing Argo CD UI via LoadBalancer..."
          kubectl patch svc argocd-server -n argocd -p '{"spec": {"type": "LoadBalancer"}}'

      - name: Apply Argo CD root application
        run: |
          echo "Applying root-app Argo CD application..."
          kubectl apply -f gitops/argocd/root-app.yaml
          
          echo "Waiting for root-app to sync..."
          # Give Argo CD a moment to pick up the new file
          sleep 20
          kubectl wait --for=condition=healthy application/root-app -n argocd --timeout=300s || echo "Wait timed out, but proceeding to status check..."

      - name: Final Status and Credentials
        run: |
          echo "--- Deployment Summary ---"
          echo "Argo CD URL:"
          kubectl get svc argocd-server -n argocd -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'
          echo ""
          echo "Argo CD Admin Password:"
          kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d
          echo ""
          echo "Nginx App URL:"
          kubectl get svc nginx-hello -n apps -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' || echo "Pending..."