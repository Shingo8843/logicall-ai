name: Deploy EKS Cluster and Argo CD

on:
  push:
    branches:
      - main
  workflow_dispatch:

permissions:
  id-token: write # Required for OIDC
  contents: read

env:
  AWS_REGION: ${{ vars.AWS_REGION || 'us-east-1' }}
  CLUSTER_NAME: ${{ vars.CLUSTER_NAME || 'logicall-ai-cluster' }}
  AWS_ROLE_ARN: ${{ vars.AWS_ROLE_ARN }}

jobs:
  deploy:
    runs-on: ubuntu-latest
    environment: AWS
    if: github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.AWS_ROLE_ARN }}
          role-session-name: GitHubActions-Deploy
          aws-region: ${{ env.AWS_REGION }}
          
      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0

      - name: Terraform Init - EKS
        working-directory: infra/eks
        run: terraform init

      - name: Import existing resources (if any)
        working-directory: infra/eks
        continue-on-error: true
        run: |
          echo "Checking for existing resources to import..."
          
          # Import CloudWatch log group (always try, ignore if already in state)
          echo "Attempting to import CloudWatch log group..."
          terraform import aws_cloudwatch_log_group.cluster /aws/eks/${{ env.CLUSTER_NAME }}/cluster 2>&1 || echo "Log group import skipped (may already be in state or not exist)"
          
          # Import IAM roles (always try, ignore if already in state)
          echo "Attempting to import IAM roles..."
          terraform import aws_iam_role.cluster ${{ env.CLUSTER_NAME }}-cluster-role 2>&1 || echo "Cluster role import skipped"
          terraform import aws_iam_role.node_group ${{ env.CLUSTER_NAME }}-node-group-role 2>&1 || echo "Node group role import skipped"
          
          # Import EKS cluster if it exists
          CLUSTER_STATUS=$(aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }} --query 'cluster.status' --output text 2>/dev/null || echo "NOT_FOUND")
          if [ "$CLUSTER_STATUS" = "ACTIVE" ]; then
            echo "Cluster is ACTIVE, importing cluster and VPC resources..."
            terraform import aws_eks_cluster.main ${{ env.CLUSTER_NAME }} 2>&1 || echo "Cluster import skipped"
            
            # Get VPC ID from cluster
            VPC_ID=$(aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }} --query 'cluster.resourcesVpcConfig.vpcId' --output text)
            echo "VPC ID: $VPC_ID"
            
            # Import VPC
            terraform import 'module.vpc.aws_vpc.this[0]' $VPC_ID 2>&1 || echo "VPC import skipped"
            
            # Import subnets by CIDR block (most reliable)
            echo "Importing subnets..."
            for SUBNET_ID in $(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" --region ${{ env.AWS_REGION }} --query 'Subnets[*].SubnetId' --output text); do
              CIDR=$(aws ec2 describe-subnets --subnet-ids $SUBNET_ID --region ${{ env.AWS_REGION }} --query 'Subnets[0].CidrBlock' --output text)
              case "$CIDR" in
                "10.0.1.0/24")
                  terraform import 'module.vpc.aws_subnet.public[0]' $SUBNET_ID 2>&1 || echo "Public subnet 0 import skipped"
                  ;;
                "10.0.2.0/24")
                  terraform import 'module.vpc.aws_subnet.public[1]' $SUBNET_ID 2>&1 || echo "Public subnet 1 import skipped"
                  ;;
                "10.0.10.0/24")
                  terraform import 'module.vpc.aws_subnet.private[0]' $SUBNET_ID 2>&1 || echo "Private subnet 0 import skipped"
                  ;;
                "10.0.20.0/24")
                  terraform import 'module.vpc.aws_subnet.private[1]' $SUBNET_ID 2>&1 || echo "Private subnet 1 import skipped"
                  ;;
              esac
            done
            
            # Import Internet Gateway
            IGW_ID=$(aws ec2 describe-internet-gateways --filters "Name=attachment.vpc-id,Values=$VPC_ID" --region ${{ env.AWS_REGION }} --query 'InternetGateways[0].InternetGatewayId' --output text 2>/dev/null)
            if [ "$IGW_ID" != "None" ] && [ -n "$IGW_ID" ]; then
              terraform import 'module.vpc.aws_internet_gateway.this[0]' $IGW_ID 2>&1 || echo "IGW import skipped"
            fi
            
            # Import NAT Gateway and EIP
            NAT_GW=$(aws ec2 describe-nat-gateways --filter "Name=vpc-id,Values=$VPC_ID" --region ${{ env.AWS_REGION }} --query 'NatGateways[0]' --output json 2>/dev/null)
            if [ "$NAT_GW" != "null" ] && [ -n "$NAT_GW" ]; then
              NAT_ID=$(echo $NAT_GW | jq -r '.NatGatewayId' 2>/dev/null || echo "")
              EIP_ALLOC=$(echo $NAT_GW | jq -r '.NatGatewayAddresses[0].AllocationId' 2>/dev/null || echo "")
              if [ -n "$EIP_ALLOC" ] && [ "$EIP_ALLOC" != "null" ]; then
                terraform import 'module.vpc.aws_eip.nat[0]' $EIP_ALLOC 2>&1 || echo "EIP import skipped"
              fi
              if [ -n "$NAT_ID" ] && [ "$NAT_ID" != "null" ]; then
                terraform import 'module.vpc.aws_nat_gateway.this[0]' $NAT_ID 2>&1 || echo "NAT Gateway import skipped"
              fi
            fi
          else
            echo "Cluster not found or not ACTIVE (status: $CLUSTER_STATUS), skipping cluster and VPC imports"
          fi
          
          echo "Import step completed. Verifying state..."
          terraform state list | head -20 || echo "State verification completed"

      - name: Verify imports and check Terraform plan
        id: terraform-plan
        working-directory: infra/eks
        continue-on-error: true
        run: |
          echo "Checking Terraform state..."
          terraform state list | grep -E "(aws_cloudwatch_log_group|aws_iam_role|aws_eks_cluster|module.vpc)" || echo "Some resources may not be in state"
          
          echo "Running terraform plan to check for issues..."
          PLAN_OUTPUT=$(terraform plan -no-color 2>&1 || true)
          echo "$PLAN_OUTPUT"
          
          # Check if plan wants to destroy the cluster
          if echo "$PLAN_OUTPUT" | grep -q "aws_eks_cluster.main.*will be destroyed"; then
            echo "destructive=true" >> $GITHUB_OUTPUT
            echo "⚠️ WARNING: Terraform wants to destroy the cluster!"
            echo "This usually means VPC resources weren't imported correctly."
            echo "Aborting to prevent cluster destruction."
            exit 1
          fi
          
          # Check if plan wants to create resources that should already exist
          if echo "$PLAN_OUTPUT" | grep -q "aws_cloudwatch_log_group.cluster.*will be created"; then
            echo "⚠️ WARNING: Log group not imported, will be created (may fail if exists)"
          fi
          if echo "$PLAN_OUTPUT" | grep -q "aws_iam_role.cluster.*will be created"; then
            echo "⚠️ WARNING: Cluster IAM role not imported, will be created (may fail if exists)"
          fi
          if echo "$PLAN_OUTPUT" | grep -q "aws_iam_role.node_group.*will be created"; then
            echo "⚠️ WARNING: Node group IAM role not imported, will be created (may fail if exists)"
          fi
          
          echo "destructive=false" >> $GITHUB_OUTPUT
          echo "Plan check completed"

      - name: Terraform Apply - EKS
        working-directory: infra/eks
        if: steps.terraform-plan.outputs.destructive != 'true'
        timeout-minutes: 45
        run: terraform apply -auto-approve

      - name: Configure kubectl
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}
          kubectl cluster-info

      - name: Setup Helm
        uses: azure/setup-helm@v3
        with:
          version: '3.13.0'

      - name: Install AWS Load Balancer Controller
        run: |
          # Note: IAM role for AWS Load Balancer Controller should be created via Terraform
          # This step installs the controller using Helm
          helm repo add eks https://aws.github.io/eks-charts
          helm repo update
          
          # Create service account (IAM role will be attached via Terraform)
          kubectl create serviceaccount aws-load-balancer-controller -n kube-system --dry-run=client -o yaml | kubectl apply -f -
          
          helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
            -n kube-system \
            --set clusterName=${{ env.CLUSTER_NAME }} \
            --set serviceAccount.create=false \
            --set serviceAccount.name=aws-load-balancer-controller \
            --set region=${{ env.AWS_REGION }} \
            --wait

      - name: Install External Secrets Operator
        run: |
          helm repo add external-secrets https://charts.external-secrets.io
          helm repo update
          
          helm upgrade --install external-secrets external-secrets/external-secrets \
            -n external-secrets \
            --create-namespace \
            --set installCRDs=true \
            --wait

      - name: Install Argo CD
        run: |
          kubectl create namespace argocd --dry-run=client -o yaml | kubectl apply -f -
          kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
          
          echo "Waiting for Argo CD to be ready..."
          kubectl wait --for=condition=available deployment/argocd-server -n argocd --timeout=300s
          
          echo "Exposing Argo CD UI via LoadBalancer..."
          kubectl patch svc argocd-server -n argocd -p '{"spec": {"type": "LoadBalancer"}}'

      - name: Apply Argo CD root application
        run: |
          echo "Applying root-app Argo CD application..."
          kubectl apply -f gitops/argocd/root-app.yaml
          
          echo "Waiting for root-app to sync..."
          # Give Argo CD a moment to pick up the new file
          sleep 20
          kubectl wait --for=condition=healthy application/root-app -n argocd --timeout=300s || echo "Wait timed out, but proceeding to status check..."

      - name: Final Status and Credentials
        run: |
          echo "--- Deployment Summary ---"
          echo "Argo CD URL:"
          kubectl get svc argocd-server -n argocd -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'
          echo ""
          echo "Argo CD Admin Password:"
          kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d
          echo ""
          echo "Nginx App URL:"
          kubectl get svc nginx-hello -n apps -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' || echo "Pending..."